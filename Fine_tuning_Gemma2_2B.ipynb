{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-23T18:06:20.439650Z",
     "start_time": "2025-10-23T18:06:20.435648Z"
    }
   },
   "source": [
    "from dataclasses import field\n",
    "\n",
    "import torch\n",
    "from gmpy2 import trunc\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from tenacity import retry_if_exception_message\n",
    "from tokenizers.trainers import Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:04:01.004881Z",
     "start_time": "2025-10-23T18:04:00.497034Z"
    }
   },
   "cell_type": "code",
   "source": "from datasets import load_dataset",
   "id": "594f8d042d0a8a7f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:04:08.259055Z",
     "start_time": "2025-10-23T18:04:01.015886Z"
    }
   },
   "cell_type": "code",
   "source": "squad_dataset = load_dataset(\"squad\")",
   "id": "c741024454c78828",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:04:14.607152Z",
     "start_time": "2025-10-23T18:04:14.599147Z"
    }
   },
   "cell_type": "code",
   "source": "squad_dataset",
   "id": "ea7df5c6d5fbda1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:04:16.840799Z",
     "start_time": "2025-10-23T18:04:16.835794Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset = squad_dataset['train']",
   "id": "76ed5243856930f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:04:18.535796Z",
     "start_time": "2025-10-23T18:04:18.529796Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset",
   "id": "edb315052609f89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:04:19.311796Z",
     "start_time": "2025-10-23T18:04:19.305799Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset = squad_dataset['validation']",
   "id": "44fdea3de7322572",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:04:19.410799Z",
     "start_time": "2025-10-23T18:04:19.404795Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset",
   "id": "84761746a7b50f89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 10570\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:04:26.716658Z",
     "start_time": "2025-10-23T18:04:25.612190Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForCausalLM.from_pretrained('google/gemma-2-2b')",
   "id": "7144721c7fc8d4f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cd54384340b4e24b9fc0aa6f85600a4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:06:55.238859Z",
     "start_time": "2025-10-23T18:06:55.231860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Quantization Config (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")"
   ],
   "id": "8bfbb153adc922da",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:07:33.781790Z",
     "start_time": "2025-10-23T18:07:26.805368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the Model in 4 Bit Precision\n",
    "model= AutoModelForCausalLM.from_pretrained('google/gemma-2-2b', config=bnb_config, device_map = 'auto')"
   ],
   "id": "e70c596351673d48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fee408405caf4009b3428ee1406311c0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:08:35.047451Z",
     "start_time": "2025-10-23T18:08:33.359220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "a4ee07d45b52aaf9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:08:47.358120Z",
     "start_time": "2025-10-23T18:08:47.353123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the Data\n",
    "def format_data(example):\n",
    "    prompt = f\"Answer the question based on the context.\\n\\nContext: {example['context']}\\n\\nQuestion: {example['question']}\\n\\nAnswer:\"\n",
    "    answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "    return {'prompt': prompt, 'answer': answer}"
   ],
   "id": "d079c8f7cfc3e5f1",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:08:51.663287Z",
     "start_time": "2025-10-23T18:08:51.547483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = train_dataset.map(format_data)\n",
    "test_dataset = test_dataset.map(format_data)"
   ],
   "id": "a20354b03621e597",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:08:53.391802Z",
     "start_time": "2025-10-23T18:08:53.385799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(example):\n",
    "    input_text = example['prompt']\n",
    "    target_text = example['answer']\n",
    "    input_ids = tokenizer(\n",
    "        input_text,\n",
    "        padding= 'max_length',\n",
    "        truncation=True,\n",
    "        max_length =512\n",
    "    )['input_ids']\n",
    "    labels = tokenizer(\n",
    "        target_text,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length=128,\n",
    "    )['input_ids']\n",
    "    return {\n",
    "        'input_ids':input_ids,\n",
    "        'labels':labels\n",
    "    }"
   ],
   "id": "a25b13e6f2102d08",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:09:00.786512Z",
     "start_time": "2025-10-23T18:08:56.798936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset_tokenized = train_dataset.map(tokenize_function,batched=True,remove_columns=squad_dataset['train'].column_names)\n",
    "test_dataset_tokenized = test_dataset.map(tokenize_function, batched= True, remove_columns = squad_dataset['validation'].column_names)"
   ],
   "id": "31014ec3a0f98f61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c37d34d4a92f4d7497bc55f5af2bada4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:09:05.575064Z",
     "start_time": "2025-10-23T18:09:05.568064Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset_tokenized",
   "id": "71db2c94d2779a2b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'answer', 'input_ids', 'labels'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7cc2a9cca3b2ec8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:09:31.128635Z",
     "start_time": "2025-10-23T18:09:28.958936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create LoRA Configuration object\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj','k_proj'],\n",
    "    modules_to_save=['lm_head'],\n",
    "    r = 8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model,lora_config)"
   ],
   "id": "71014a0648f339bb",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:09:35.029423Z",
     "start_time": "2025-10-23T18:09:34.904905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the Trainnig Arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './lora-squad-gemma2b',\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size = 2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs = 3,\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    eval_strategy='steps',\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    report_to='none'\n",
    ")"
   ],
   "id": "3b9a82be997fce87",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:11:49.733781Z",
     "start_time": "2025-10-23T18:11:49.725274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Minimizing the Dataset\n",
    "small_train_dataset = train_dataset_tokenized.select(range(2000))\n",
    "small_test_dataset = test_dataset_tokenized.select(range(300))"
   ],
   "id": "4a9982cd0819eef4",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:12:07.218188Z",
     "start_time": "2025-10-23T18:12:07.134039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    ")"
   ],
   "id": "79ede5ef7dc6525e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:12:44.281080Z",
     "start_time": "2025-10-23T18:12:21.911047Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "87f582ac122f4fb5",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1000.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 17.42 GiB is allocated by PyTorch, and 344.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\trainer.py:2325\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2323\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2324\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2326\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2327\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2328\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2329\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2330\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\trainer.py:2674\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2667\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2668\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2669\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2670\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[0;32m   2671\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2672\u001B[0m )\n\u001B[0;32m   2673\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2674\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2676\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2677\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2678\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2679\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2680\u001B[0m ):\n\u001B[0;32m   2681\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2682\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\trainer.py:4020\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   4017\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m   4019\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[1;32m-> 4020\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4022\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[0;32m   4023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   4024\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   4025\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   4026\u001B[0m ):\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\trainer.py:4110\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   4108\u001B[0m         kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items_in_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_items_in_batch\n\u001B[0;32m   4109\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[1;32m-> 4110\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[0;32m   4111\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[0;32m   4112\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[0;32m   4113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\accelerate\\utils\\operations.py:819\u001B[0m, in \u001B[0;36mconvert_outputs_to_fp32.<locals>.forward\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 819\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\accelerate\\utils\\operations.py:807\u001B[0m, in \u001B[0;36mConvertOutputsToFp32.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    806\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 807\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001B[0m, in \u001B[0;36mautocast_decorator.<locals>.decorate_autocast\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_autocast\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[1;32m---> 44\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\peft\\peft_model.py:1850\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.forward\u001B[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n\u001B[0;32m   1848\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1849\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n\u001B[1;32m-> 1850\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model(\n\u001B[0;32m   1851\u001B[0m             input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   1852\u001B[0m             attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   1853\u001B[0m             inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[0;32m   1854\u001B[0m             labels\u001B[38;5;241m=\u001B[39mlabels,\n\u001B[0;32m   1855\u001B[0m             output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m   1856\u001B[0m             output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[0;32m   1857\u001B[0m             return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m   1858\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1859\u001B[0m         )\n\u001B[0;32m   1861\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n\u001B[0;32m   1862\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1863\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:222\u001B[0m, in \u001B[0;36mBaseTuner.forward\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[1;32m--> 222\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\accelerate\\hooks.py:175\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[1;34m(module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    173\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 175\u001B[0m     output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\utils\\generic.py:918\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    917\u001B[0m     return_dict \u001B[38;5;241m=\u001B[39m return_dict_passed\n\u001B[1;32m--> 918\u001B[0m output \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    920\u001B[0m     output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:573\u001B[0m, in \u001B[0;36mGemma2ForCausalLM.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[0;32m    571\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 573\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_function(logits, labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_size, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m CausalLMOutputWithPast(\n\u001B[0;32m    576\u001B[0m     loss\u001B[38;5;241m=\u001B[39mloss,\n\u001B[0;32m    577\u001B[0m     logits\u001B[38;5;241m=\u001B[39mlogits,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    580\u001B[0m     attentions\u001B[38;5;241m=\u001B[39moutputs\u001B[38;5;241m.\u001B[39mattentions,\n\u001B[0;32m    581\u001B[0m )\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\loss\\loss_utils.py:55\u001B[0m, in \u001B[0;36mForCausalLMLoss\u001B[1;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mForCausalLMLoss\u001B[39m(\n\u001B[0;32m     46\u001B[0m     logits,\n\u001B[0;32m     47\u001B[0m     labels,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     53\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;66;03m# Upcast to float if we need to compute the loss to avoid potential precision issues\u001B[39;00m\n\u001B[1;32m---> 55\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m shift_labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     58\u001B[0m         \u001B[38;5;66;03m# Shift so that tokens < n predict n\u001B[39;00m\n\u001B[0;32m     59\u001B[0m         labels \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mpad(labels, (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m), value\u001B[38;5;241m=\u001B[39mignore_index)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1000.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 17.42 GiB is allocated by PyTorch, and 344.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:00:17.169421Z",
     "start_time": "2025-10-23T17:59:16.574169Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers accelerate peft bitsandbytes\n",
   "id": "631979dd84e852f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: peft in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (0.17.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: networkx in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dawood khan\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-win_amd64.whl (59.5 MB)\n",
      "   ---------------------------------------- 0.0/59.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/59.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.3/59.5 MB 4.2 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 1.8/59.5 MB 4.2 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 2.1/59.5 MB 3.5 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 2.6/59.5 MB 2.7 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 2.9/59.5 MB 2.8 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 2.9/59.5 MB 2.8 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 2.9/59.5 MB 2.8 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 2.9/59.5 MB 2.8 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 2.9/59.5 MB 2.8 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 3.1/59.5 MB 1.4 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 3.7/59.5 MB 1.5 MB/s eta 0:00:37\n",
      "   -- ------------------------------------- 4.5/59.5 MB 1.7 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 5.2/59.5 MB 1.9 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 5.2/59.5 MB 1.9 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 5.2/59.5 MB 1.9 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 5.2/59.5 MB 1.9 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 5.2/59.5 MB 1.9 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 5.5/59.5 MB 1.4 MB/s eta 0:00:38\n",
      "   --- ------------------------------------ 5.5/59.5 MB 1.4 MB/s eta 0:00:38\n",
      "   --- ------------------------------------ 5.8/59.5 MB 1.3 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 5.8/59.5 MB 1.3 MB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 6.3/59.5 MB 1.4 MB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 6.3/59.5 MB 1.4 MB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.6/59.5 MB 1.3 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 6.8/59.5 MB 854.2 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 7.1/59.5 MB 695.7 kB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 7.1/59.5 MB 695.7 kB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 7.1/59.5 MB 695.7 kB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 7.3/59.5 MB 672.1 kB/s eta 0:01:18\n",
      "   ---- ----------------------------------- 7.3/59.5 MB 672.1 kB/s eta 0:01:18\n",
      "   ----- ---------------------------------- 7.6/59.5 MB 674.9 kB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 7.6/59.5 MB 674.9 kB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 7.6/59.5 MB 674.9 kB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 7.6/59.5 MB 674.9 kB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 7.6/59.5 MB 674.9 kB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 7.6/59.5 MB 674.9 kB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 7.6/59.5 MB 674.9 kB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 7.9/59.5 MB 617.4 kB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 7.9/59.5 MB 617.4 kB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 7.9/59.5 MB 617.4 kB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 7.9/59.5 MB 617.4 kB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 7.9/59.5 MB 617.4 kB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 7.9/59.5 MB 617.4 kB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 7.9/59.5 MB 617.4 kB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 7.9/59.5 MB 617.4 kB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 8.1/59.5 MB 563.6 kB/s eta 0:01:32\n",
      "   ----- ---------------------------------- 8.1/59.5 MB 563.6 kB/s eta 0:01:32\n",
      "   ----- ---------------------------------- 8.1/59.5 MB 563.6 kB/s eta 0:01:32\n",
      "   ----- ---------------------------------- 8.1/59.5 MB 563.6 kB/s eta 0:01:32\n",
      "   ----- ---------------------------------- 8.1/59.5 MB 563.6 kB/s eta 0:01:32\n",
      "   ----- ---------------------------------- 8.1/59.5 MB 563.6 kB/s eta 0:01:32\n",
      "   ----- ---------------------------------- 8.4/59.5 MB 539.0 kB/s eta 0:01:35\n",
      "   ----- ---------------------------------- 8.4/59.5 MB 539.0 kB/s eta 0:01:35\n",
      "   ----- ---------------------------------- 8.4/59.5 MB 539.0 kB/s eta 0:01:35\n",
      "   ------ --------------------------------- 9.2/59.5 MB 564.2 kB/s eta 0:01:30\n",
      "   ------ --------------------------------- 10.2/59.5 MB 623.8 kB/s eta 0:01:20\n",
      "   ------- -------------------------------- 10.5/59.5 MB 637.1 kB/s eta 0:01:17\n",
      "   ------- -------------------------------- 10.5/59.5 MB 637.1 kB/s eta 0:01:17\n",
      "   ------- -------------------------------- 10.5/59.5 MB 637.1 kB/s eta 0:01:17\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 10.7/59.5 MB 627.2 kB/s eta 0:01:18\n",
      "   ------- -------------------------------- 11.0/59.5 MB 572.7 kB/s eta 0:01:25\n",
      "   ------- -------------------------------- 11.0/59.5 MB 572.7 kB/s eta 0:01:25\n",
      "   ------- -------------------------------- 11.3/59.5 MB 572.4 kB/s eta 0:01:25\n",
      "   ------- -------------------------------- 11.3/59.5 MB 572.4 kB/s eta 0:01:25\n",
      "   ------- -------------------------------- 11.5/59.5 MB 573.9 kB/s eta 0:01:24\n",
      "   -------- ------------------------------- 12.1/59.5 MB 594.5 kB/s eta 0:01:20\n",
      "   -------- ------------------------------- 12.6/59.5 MB 615.1 kB/s eta 0:01:17\n",
      "   -------- ------------------------------- 13.4/59.5 MB 647.8 kB/s eta 0:01:12\n",
      "   --------- ------------------------------ 13.6/59.5 MB 656.2 kB/s eta 0:01:10\n",
      "   --------- ------------------------------ 14.4/59.5 MB 685.8 kB/s eta 0:01:06\n",
      "   --------- ------------------------------ 14.7/59.5 MB 693.8 kB/s eta 0:01:05\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 14.9/59.5 MB 698.0 kB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 15.2/59.5 MB 624.2 kB/s eta 0:01:12\n",
      "   ---------- ----------------------------- 15.5/59.5 MB 627.4 kB/s eta 0:01:11\n",
      "   ---------- ----------------------------- 15.7/59.5 MB 632.1 kB/s eta 0:01:10\n",
      "   ---------- ----------------------------- 16.0/59.5 MB 637.9 kB/s eta 0:01:09\n",
      "   ---------- ----------------------------- 16.3/59.5 MB 644.9 kB/s eta 0:01:08\n",
      "   ----------- ---------------------------- 16.8/59.5 MB 658.9 kB/s eta 0:01:05\n",
      "   ----------- ---------------------------- 17.3/59.5 MB 674.4 kB/s eta 0:01:03\n",
      "   ----------- ---------------------------- 17.8/59.5 MB 690.4 kB/s eta 0:01:01\n",
      "   ------------ --------------------------- 18.6/59.5 MB 715.7 kB/s eta 0:00:58\n",
      "   ------------ --------------------------- 18.9/59.5 MB 723.3 kB/s eta 0:00:57\n",
      "   ------------- -------------------------- 19.7/59.5 MB 743.0 kB/s eta 0:00:54\n",
      "   ------------- -------------------------- 19.9/59.5 MB 750.3 kB/s eta 0:00:53\n",
      "   ------------- -------------------------- 20.2/59.5 MB 755.8 kB/s eta 0:00:53\n",
      "   ------------- -------------------------- 20.7/59.5 MB 765.7 kB/s eta 0:00:51\n",
      "   -------------- ------------------------- 21.0/59.5 MB 769.7 kB/s eta 0:00:51\n",
      "   -------------- ------------------------- 21.2/59.5 MB 773.6 kB/s eta 0:00:50\n",
      "   -------------- ------------------------- 21.5/59.5 MB 777.4 kB/s eta 0:00:49\n",
      "   -------------- ------------------------- 21.8/59.5 MB 781.7 kB/s eta 0:00:49\n",
      "   -------------- ------------------------- 22.0/59.5 MB 786.3 kB/s eta 0:00:48\n",
      "   -------------- ------------------------- 22.3/59.5 MB 790.8 kB/s eta 0:00:48\n",
      "   --------------- ------------------------ 22.5/59.5 MB 795.3 kB/s eta 0:00:47\n",
      "   --------------- ------------------------ 22.8/59.5 MB 799.8 kB/s eta 0:00:46\n",
      "   --------------- ------------------------ 23.3/59.5 MB 809.9 kB/s eta 0:00:45\n",
      "   --------------- ------------------------ 23.6/59.5 MB 815.0 kB/s eta 0:00:45\n",
      "   ---------------- ----------------------- 23.9/59.5 MB 819.7 kB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 24.1/59.5 MB 823.5 kB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 24.4/59.5 MB 827.2 kB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 24.9/59.5 MB 834.4 kB/s eta 0:00:42\n",
      "   ---------------- ----------------------- 25.2/59.5 MB 838.4 kB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 25.4/59.5 MB 842.8 kB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 25.7/59.5 MB 840.6 kB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 26.2/59.5 MB 822.7 kB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 26.5/59.5 MB 814.3 kB/s eta 0:00:41\n",
      "   ------------------ --------------------- 27.0/59.5 MB 822.3 kB/s eta 0:00:40\n",
      "   ------------------ --------------------- 27.3/59.5 MB 814.8 kB/s eta 0:00:40\n",
      "   ------------------ --------------------- 27.8/59.5 MB 846.5 kB/s eta 0:00:38\n",
      "   ------------------ --------------------- 28.0/59.5 MB 851.4 kB/s eta 0:00:37\n",
      "   ------------------- -------------------- 28.6/59.5 MB 861.5 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 28.8/59.5 MB 867.2 kB/s eta 0:00:36\n",
      "   ------------------- -------------------- 29.4/59.5 MB 877.5 kB/s eta 0:00:35\n",
      "   -------------------- ------------------- 29.9/59.5 MB 875.6 kB/s eta 0:00:34\n",
      "   -------------------- ------------------- 30.4/59.5 MB 873.8 kB/s eta 0:00:34\n",
      "   -------------------- ------------------- 30.7/59.5 MB 866.4 kB/s eta 0:00:34\n",
      "   -------------------- ------------------- 31.2/59.5 MB 884.9 kB/s eta 0:00:33\n",
      "   --------------------- ------------------ 31.7/59.5 MB 894.8 kB/s eta 0:00:32\n",
      "   --------------------- ------------------ 32.2/59.5 MB 905.4 kB/s eta 0:00:31\n",
      "   --------------------- ------------------ 32.5/59.5 MB 909.5 kB/s eta 0:00:30\n",
      "   --------------------- ------------------ 32.5/59.5 MB 909.5 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 32.8/59.5 MB 910.2 kB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 33.0/59.5 MB 915.4 kB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 33.3/59.5 MB 915.4 kB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 33.6/59.5 MB 917.5 kB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 33.8/59.5 MB 916.9 kB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 34.1/59.5 MB 919.4 kB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 34.3/59.5 MB 1.0 MB/s eta 0:00:25\n",
      "   ----------------------- ---------------- 34.6/59.5 MB 1.0 MB/s eta 0:00:25\n",
      "   ----------------------- ---------------- 34.9/59.5 MB 1.0 MB/s eta 0:00:25\n",
      "   ----------------------- ---------------- 35.4/59.5 MB 1.0 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 35.7/59.5 MB 1.0 MB/s eta 0:00:24\n",
      "   ------------------------ --------------- 35.9/59.5 MB 1.0 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 36.4/59.5 MB 1.0 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 36.7/59.5 MB 1.0 MB/s eta 0:00:22\n",
      "   ------------------------- -------------- 37.2/59.5 MB 1.1 MB/s eta 0:00:22\n",
      "   ------------------------- -------------- 37.7/59.5 MB 1.1 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 38.0/59.5 MB 1.1 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 38.5/59.5 MB 1.1 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 39.1/59.5 MB 1.1 MB/s eta 0:00:19\n",
      "   -------------------------- ------------- 39.1/59.5 MB 1.1 MB/s eta 0:00:19\n",
      "   -------------------------- ------------- 39.6/59.5 MB 1.1 MB/s eta 0:00:19\n",
      "   -------------------------- ------------- 39.8/59.5 MB 1.2 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 40.1/59.5 MB 1.2 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 40.1/59.5 MB 1.2 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 40.4/59.5 MB 1.2 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 40.4/59.5 MB 1.2 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 40.4/59.5 MB 1.2 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 40.6/59.5 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 40.9/59.5 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 40.9/59.5 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 41.2/59.5 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 41.4/59.5 MB 1.2 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 41.4/59.5 MB 1.2 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 41.7/59.5 MB 1.2 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 41.9/59.5 MB 1.2 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 42.2/59.5 MB 1.2 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 42.5/59.5 MB 1.2 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 42.7/59.5 MB 1.2 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 43.0/59.5 MB 1.2 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 43.3/59.5 MB 1.2 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 43.5/59.5 MB 1.2 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 44.0/59.5 MB 1.2 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 44.0/59.5 MB 1.2 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 44.0/59.5 MB 1.2 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 44.0/59.5 MB 1.2 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 44.0/59.5 MB 1.2 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 44.0/59.5 MB 1.2 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 44.3/59.5 MB 1.2 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 44.8/59.5 MB 1.2 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 45.1/59.5 MB 1.2 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 45.6/59.5 MB 1.3 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 45.9/59.5 MB 1.3 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 46.4/59.5 MB 1.3 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 46.9/59.5 MB 1.3 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 47.2/59.5 MB 1.3 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 47.4/59.5 MB 1.3 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 47.4/59.5 MB 1.3 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 47.7/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 47.7/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 47.7/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.0/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.0/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.2/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.2/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.2/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.2/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.5/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 48.8/59.5 MB 1.3 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 49.0/59.5 MB 1.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 49.3/59.5 MB 1.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 49.5/59.5 MB 1.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 49.8/59.5 MB 1.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.1/59.5 MB 1.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.1/59.5 MB 1.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.3/59.5 MB 1.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.3/59.5 MB 1.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.3/59.5 MB 1.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 50.6/59.5 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 50.6/59.5 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 50.6/59.5 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 50.6/59.5 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 50.6/59.5 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 50.6/59.5 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 50.9/59.5 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 50.9/59.5 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 50.9/59.5 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 51.1/59.5 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 51.1/59.5 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 51.1/59.5 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 51.6/59.5 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 51.9/59.5 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 52.2/59.5 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 52.4/59.5 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 53.0/59.5 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 53.2/59.5 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 53.7/59.5 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 54.0/59.5 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 54.5/59.5 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 54.8/59.5 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 55.3/59.5 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 55.8/59.5 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 56.1/59.5 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 56.6/59.5 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 57.1/59.5 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 57.7/59.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  58.2/59.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  58.7/59.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  59.0/59.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  59.5/59.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 59.5/59.5 MB 1.4 MB/s  0:00:55\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.1\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1701f015e153ad29"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
