{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:17:11.673067Z",
     "start_time": "2025-10-27T19:16:49.157138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,BitsAndBytesConfig, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers"
   ],
   "id": "313ebb82ec6cf8ea",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:17:11.696067Z",
     "start_time": "2025-10-27T19:17:11.691063Z"
    }
   },
   "cell_type": "code",
   "source": "model_name = 'google/gemma-2-2b'",
   "id": "40ebe1890098ffba",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:17:11.713067Z",
     "start_time": "2025-10-27T19:17:11.706066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")"
   ],
   "id": "b114eb88b36d2d99",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:17:40.286305Z",
     "start_time": "2025-10-27T19:17:11.731065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map = 'auto',\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision = 'main')"
   ],
   "id": "205f761e3b9e5b7f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dc00530da4b435e99da043fb7169498"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:18:43.888335Z",
     "start_time": "2025-10-27T19:18:42.289069Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)",
   "id": "7ab92524b32ba94d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:42:58.227526Z",
     "start_time": "2025-10-27T18:41:01.698381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using base model for predictions\n",
    "model.eval() # It de-activates the dropout modules\n",
    "\n",
    "comment = 'It was a good video, Thanks'\n",
    "prompt = f'''<bos>{comment}<eos>'''\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids = inputs['input_ids'].to('cuda'), max_new_tokens = 132)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ],
   "id": "ae9aa46a2295a388",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos>It was a good video, Thanks<eos>I have a question. I have a 2006 325i with 110k miles. I have a check engine light on and the car is running rough. I have a code P0300. I have replaced the spark plugs and coils. I have also replaced the fuel filter. I have also replaced the fuel pump. I have also replaced the fuel pressure regulator. I have also replaced the fuel injectors. I have also replaced the fuel pressure sensor. I have also replaced the fuel pump relay. I have also replaced the fuel pump relay. I have also replaced the fuel pump relay. I have also replaced\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:44:11.895665Z",
     "start_time": "2025-10-27T18:44:11.890667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instructions = '''Dawood Khan, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "It reacts to feedback aptly and ends responses with its signature '-Dawood'. \\\n",
    "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.'''"
   ],
   "id": "351c409332e3b359",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:46:13.153942Z",
     "start_time": "2025-10-27T18:46:13.150942Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_temp= lambda comment: f'''<bos>{instructions}\\n{comment}\\n <eos>'''",
   "id": "4837fff568cb6cbc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:46:30.860036Z",
     "start_time": "2025-10-27T18:46:30.857037Z"
    }
   },
   "cell_type": "code",
   "source": "prompt = prompt_temp(comment)",
   "id": "37712ff557bd5279",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:46:55.414943Z",
     "start_time": "2025-10-27T18:46:55.410438Z"
    }
   },
   "cell_type": "code",
   "source": "print(prompt)",
   "id": "24e4fd6230a7780a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Dawood Khan, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '-Dawood'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      "It was a good video, Thanks\n",
      " <eos>\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:49:52.283646Z",
     "start_time": "2025-10-27T18:47:58.497841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = tokenizer(prompt, return_tensors = 'pt')\n",
    "\n",
    "outputs = model.generate(input_ids = inputs['input_ids'].to('cuda'), max_new_tokens = 132)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ],
   "id": "fbc516237dc862a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos>Dawood Khan, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '-Dawood'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      "It was a good video, Thanks\n",
      " <eos> \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:51:28.045526Z",
     "start_time": "2025-10-27T18:51:28.041526Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Preparing Model FOr Training",
   "id": "ac8db30c980c9473"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:18:52.092446Z",
     "start_time": "2025-10-27T19:18:52.062427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.train() # Now we activate the dropout modules\n",
    "model.gradient_checkpointing_enable() # gradient checkpoint enabled\n",
    "model.config.use_cache = False\n",
    "model.enable_input_require_grads()\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "id": "c5c477c8bb30baaf",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:18:56.876901Z",
     "start_time": "2025-10-27T19:18:56.870899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuring LoRA\n",
    "config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.04,\n",
    "    bias = 'none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['q_proj']\n",
    ")"
   ],
   "id": "e61c512dd9f8c80b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:18:57.278108Z",
     "start_time": "2025-10-27T19:18:57.192437Z"
    }
   },
   "cell_type": "code",
   "source": "model = get_peft_model(model,config)",
   "id": "530f45c4f1c55133",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:18:58.738884Z",
     "start_time": "2025-10-27T19:18:58.729883Z"
    }
   },
   "cell_type": "code",
   "source": "model.print_trainable_parameters()",
   "id": "b08d6ce8bf5f03ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 905,216 || all params: 2,615,247,104 || trainable%: 0.0346\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now Prepare Dataset",
   "id": "e24b0a6dabb09396"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:19:08.386487Z",
     "start_time": "2025-10-27T19:19:03.227023Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"shawhin/shawgpt-youtube-comments\")",
   "id": "772c837615419879",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:19:12.405450Z",
     "start_time": "2025-10-27T19:19:12.398451Z"
    }
   },
   "cell_type": "code",
   "source": "dataset['train']",
   "id": "5ea48145f4a7a689",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['example'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:19:15.864789Z",
     "start_time": "2025-10-27T19:19:15.859780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we define the tokenize function\n",
    "def tokenize (example):\n",
    "    text = example['example']\n",
    "\n",
    "    tokenizer.truncation_side= 'left'\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors = 'np',\n",
    "        truncation = True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    return tokenized_inputs\n"
   ],
   "id": "53df7344e3c8e516",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:19:18.679032Z",
     "start_time": "2025-10-27T19:19:17.816715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize,batched=True)"
   ],
   "id": "38f1fa6a8cd0644b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fceb42995a1948dba923dc3528c5b300"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:19:21.153192Z",
     "start_time": "2025-10-27T19:19:21.148683Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.pad_token = tokenizer.eos_token",
   "id": "9f890508d5fe1b13",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:19:21.825137Z",
     "start_time": "2025-10-27T19:19:21.820135Z"
    }
   },
   "cell_type": "code",
   "source": "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm = False)",
   "id": "15c7d14393c21f24",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine Tuning The Model",
   "id": "2f7f76002ed1010b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:19:25.139195Z",
     "start_time": "2025-10-27T19:19:25.061686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir = './output',\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim='paged_adamw_8bit'\n",
    ")"
   ],
   "id": "6e2a01349a5adb4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:19:26.416500Z",
     "start_time": "2025-10-27T19:19:26.349978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset = tokenized_dataset['test'],\n",
    "    data_collator = data_collator,\n",
    ")"
   ],
   "id": "b0494490a1ff23b8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:24:20.075442Z",
     "start_time": "2025-10-27T19:19:35.887628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training the Model\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "model.config.use_cache = True\n"
   ],
   "id": "dd9ebeb52be8f6bc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/42 04:32, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.782400</td>\n",
       "      <td>3.508419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.451500</td>\n",
       "      <td>3.062764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>2.731298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.620700</td>\n",
       "      <td>2.416805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.350200</td>\n",
       "      <td>2.143040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.013800</td>\n",
       "      <td>1.962128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "ae7fdc3b940e6de5ce127d4d08b7895d"
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:24:57.253081Z",
     "start_time": "2025-10-27T19:24:57.233075Z"
    }
   },
   "cell_type": "code",
   "source": "model.eval()",
   "id": "fd2459e8a68dc75b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.04, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "              (act_fn): GELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:25:14.803357Z",
     "start_time": "2025-10-27T19:25:14.798356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instructions = '''Dawood Khan, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
    "It reacts to feedback aptly and ends responses with its signature '-Dawood'. \\\n",
    "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.'''"
   ],
   "id": "d31b640858639be3",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:25:53.977387Z",
     "start_time": "2025-10-27T19:25:53.972386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "comment = 'It was a good video, Thanks'\n",
    "prompt_temp= lambda comment: f'''<bos>{instructions}\\n{comment}\\n <eos>'''\n",
    "prompt = prompt_temp(comment)"
   ],
   "id": "f0404205ff415514",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:28:05.990265Z",
     "start_time": "2025-10-27T19:28:05.986753Z"
    }
   },
   "cell_type": "code",
   "source": "print(prompt)",
   "id": "a49e45cb776602c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Dawood Khan, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '-Dawood'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      "It was a good video, Thanks\n",
      " <eos>\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:26:14.290512Z",
     "start_time": "2025-10-27T19:26:14.273504Z"
    }
   },
   "cell_type": "code",
   "source": "inputs = tokenizer(prompt, return_tensors = 'pt')",
   "id": "151e3eb0d9f7293a",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:27:04.495450Z",
     "start_time": "2025-10-27T19:27:03.655575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(input_ids = inputs['input_ids'].to('cuda'), max_new_tokens = 132)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ],
   "id": "cf502f6f9da20de7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos>Dawood Khan, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '-Dawood'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
      "\n",
      "Please respond to the following comment.\n",
      "It was a good video, Thanks\n",
      " <eos><eos>\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
